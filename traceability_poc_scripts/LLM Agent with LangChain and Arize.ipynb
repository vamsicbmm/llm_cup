{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df79d13c-4811-4a5a-a605-44e56156ffa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install arize-phoenix langchain openai tiktoken\n",
    "%pip install protobuf==3.19.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764f542a-c5ed-4665-a310-5240aa567d29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from phoenix.trace.langchain import LangChainInstrumentor, OpenInferenceTracer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8950ec07-c6cf-4d5b-a4c2-fc76f53625cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbd79a7-8c60-48e0-beae-fa4526ffcc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae3482b-4c70-4d87-8ea8-685b3de1fc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\" \n",
    "openai.api_base =  \"https://kf-llm.openai.azure.com/\" # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = \"d6e77295870f4e0fb5f44e2b96838801\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"d6e77295870f4e0fb5f44e2b96838801\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6723c2-6cf6-4845-8843-df921bc3d1fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Launch phoenix\n",
    "session = px.launch_app()\n",
    "\n",
    "# Once you have started a Phoenix server, you can start your LangChain application with the OpenInferenceTracer as a callback. To do this, you will have to instrument your LangChain application with the tracer:\n",
    "\n",
    "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
    "\n",
    "# If no exporter is specified, the tracer will export to the locally running Phoenix server\n",
    "tracer = OpenInferenceTracer()\n",
    "LangChainInstrumentor(tracer).instrument()\n",
    "\n",
    "# Initialize your LangChain application\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "\n",
    "# Load embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                   model_kwargs={'device': 'cpu'})\n",
    "documents_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/langchain-pinecone/database.parquet\"\n",
    ")\n",
    "knn_retriever = KNNRetriever(\n",
    "    index=np.stack(documents_df[\"text_vector\"]),\n",
    "    texts=documents_df[\"text\"].tolist(),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-3.5-turbo\"\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"kf-gpt\",\n",
    "    model_name=\"gpt-35-turbo\",temperature= 0.3\n",
    ")\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=knn_retriever,\n",
    ")\n",
    "\n",
    "# Instrument the execution of the runs with the tracer. By default the tracer uses an HTTPExporter\n",
    "query = \"What is euclidean distance?\"\n",
    "# response = chain.run(query, callbacks=[tracer])\n",
    "\n",
    "# By adding the tracer to the callbacks of LangChain, we've created a one-way data connection between your LLM application and Phoenix.\n",
    "\n",
    "# To view the traces in Phoenix, simply open the UI in your browser.\n",
    "session.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711844ec-19bf-4602-b50d-92ae3c8ea86c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitbookLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import VectorDBQA\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_vectorstore_agent,VectorStoreToolkit,VectorStoreInfo\n",
    "from datetime import datetime\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.arize_callback import ArizeCallbackHandler\n",
    "from langchain.callbacks.base import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b87a275-e786-4ad5-8b73-4833ecaf5331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Arize doc data from gitbook\n",
    "loader = GitbookLoader(\"https://docs.arize.com/\",load_all_paths=True)\n",
    "\n",
    "pages_data = loader.load()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4205544229603981,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM Agent with LangChain and Arize",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
